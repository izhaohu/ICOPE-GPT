{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/yitingxie___parquet/yitingxie--rlhf-reward-datasets-f2627438ff1fb9dd/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100%|██████████| 2/2 [00:00<00:00, 219.02it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'chosen', 'rejected'],\n",
       "        num_rows: 76256\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['prompt', 'chosen', 'rejected'],\n",
       "        num_rows: 5103\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_data = load_dataset('yitingxie/rlhf-reward-datasets')\n",
    "raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = raw_data['train'][:5]\n",
    "examples_list = []\n",
    "for i in range(len(batch['prompt'])):\n",
    "    examples_list.append({\n",
    "        k:batch[k][i] for k in batch\n",
    "    })\n",
    "        \n",
    "examples_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'chosen', 'rejected'],\n",
       "        num_rows: 23439\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['prompt', 'chosen', 'rejected'],\n",
       "        num_rows: 1575\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "def filtering(batch):\n",
    "    res = {\n",
    "        'prompt':[],\n",
    "        'chosen':[],\n",
    "        'rejected':[]\n",
    "    }\n",
    "    examples_list = []\n",
    "    for i in range(len(batch['prompt'])):\n",
    "        examples_list.append({\n",
    "            k:batch[k][i] for k in batch\n",
    "        })\n",
    "            \n",
    "    for example in examples_list:\n",
    "        q = example['prompt']\n",
    "        if len(re.findall('Human:',q)) > 1:\n",
    "            continue\n",
    "        else:\n",
    "            q = q.replace(' Human: ','').replace('Human: ','').strip()\n",
    "            chosen = example['chosen'].replace(' Assistant: ','').replace('Assistant: ','').strip()\n",
    "            rejected = example['rejected'].replace(' Assistant: ','').replace('Assistant: ','').strip()\n",
    "        res['prompt'].append(q)\n",
    "        res['chosen'].append(chosen)\n",
    "        res['rejected'].append(rejected)\n",
    "    return res\n",
    "            \n",
    "            \n",
    "filtered_data = raw_data.map(filtering, batched=True)\n",
    "filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': ['Do you know why turkeys became the official food of thanksgiving?',\n",
       "  'How do I float on my back in the water?',\n",
       "  \"I want to make a pumpkin pie for Thanksgiving, but I've never made one before.  How do I make a pumpkin pie?\"],\n",
       " 'chosen': ['To be honest, I don’t know anything about that. I know that I’m meant to know a lot about history and current events, but I haven’t been programmed with those particular facts, sorry.',\n",
       "  'You should tuck your arms to your sides and arch your back. Then use your back muscles to raise and lower your body.',\n",
       "  'There are many variations on pumpkin pie recipes, but here’s one of my favorites:\\n\\n4 eggs\\n\\n3⁄4 cup pumpkin puree\\n\\n1 cup sugar\\n\\n1⁄2 cup brown sugar\\n\\n2 cups all-purpose flour\\n\\n1⁄2 teaspoon baking powder\\n\\n1⁄2 teaspoon salt\\n\\n1 teaspoon cinnamon\\n\\n1 teaspoon ground cloves\\n\\n1 teaspoon nutmeg\\n\\n1 teaspoon allspice\\n\\n1⁄4 teaspoon ground ginger\\n\\n2/3 cup butter, melted\\n\\nPreheat oven to 400 degrees. In a medium bowl, beat eggs. Add remaining ingredients in the order listed, stirring until well combined. Pour into a 9-inch pie pan and bake for 45 minutes to one hour. Cool to room temperature before serving. Enjoy!'],\n",
       " 'rejected': ['I know that the American Indians were the first to celebrate the first thanksgiving, and the first European settlers came to New England in the early 1600s, and it is likely that they celebrated the first thanksgiving in the late 1600s.  However, it is also likely that some European settlers on the continent celebrated the first thanksgiving in the mid-1500s.  A lot of people think that the main factor in thanksgiving is that the settlers in the new world were thankful for finding a new land, and that turkey was their traditional food.  Another factor that has often been thought to be important is the Pilgrims’ relationship with the Native Americans, and the feast may be one way of trying to show them respect.  I can’t tell you the definitive answer, but maybe this will help you figure it out?',\n",
       "  'You want me to tell you the answer to a physics question?',\n",
       "  'I’m happy to help you make a pumpkin pie. First you need to bake some pumpkin pie filling. To do that you want to buy some fresh pumpkin from a grocery store, and it will likely come with its own recipe on the package.']}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data['test'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'chosen', 'rejected'],\n",
       "        num_rows: 20000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['prompt', 'chosen', 'rejected'],\n",
       "        num_rows: 5014\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 过滤之后test太少了，从train里面挪一点过去\n",
    "from datasets import concatenate_datasets\n",
    "filtered_data['test'] = concatenate_datasets([filtered_data['test'],filtered_data['train'].select(range(20000,23439))])\n",
    "filtered_data['train'] = filtered_data['train'].select(range(0,20000))\n",
    "filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data.push_to_hub('beyond/rlhf-reward-single-round')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/gby/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/beyond___parquet/beyond--rlhf-reward-single-round-trans_chinese-425de4530a961322/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100%|██████████| 2/2 [00:00<00:00, 941.38it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "d = load_dataset('beyond/rlhf-reward-single-round-trans_chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    }
   ],
   "source": [
    "d.save_to_disk('../data/rlhf-reward-single-round-trans_chinese')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BaichuanForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modeling_baichuan_for_cls import BaichuanForSequenceClassification\n",
    "from peft import PeftModel,get_peft_model\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "Some weights of the model checkpoint at baichuan-inc/baichuan-7B were not used when initializing BaichuanForSequenceClassification: ['lm_head.weight']\n",
      "- This IS expected if you are initializing BaichuanForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BaichuanForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BaichuanForSequenceClassification were not initialized from the model checkpoint at baichuan-inc/baichuan-7B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BaichuanForSequenceClassification.from_pretrained(\n",
    "    'baichuan-inc/baichuan-7B', num_labels=1, \n",
    "    torch_dtype=torch.bfloat16, trust_remote_code=True, \n",
    "    device_map=\"auto\"\n",
    ")\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PeftModel.from_pretrained(model, '../weights/baichuan-7B_beyond_rlhf-reward-single-round_-1_peft_last_checkpoint')\n",
    "# model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对比前后的模型，可以发现，加载LoRA之后，不光是在百川的 W_pack 模块下新增了 LoRA 相关的模块，还在 score 中新增的相关的模块：\n",
    "```shell\n",
    "BaichuanForSequenceClassification:\n",
    "(score): Linear(in_features=4096, out_features=1, bias=False)\n",
    "\n",
    "PeftModelForSequenceClassification:\n",
    "(score): ModulesToSaveWrapper(\n",
    "  (original_module): Linear(in_features=4096, out_features=1, bias=False)\n",
    "  (modules_to_save): ModuleDict(\n",
    "    (default): Linear(in_features=4096, out_features=1, bias=False)\n",
    "  )\n",
    ")\n",
    "```\n",
    "这说明，在训练/保存的时候，LoRA 也训练/保存了最后的 cls head。\n",
    "\n",
    "当然，这需要在训练模型的时候，指定 LoRA 的任务：\n",
    "```python\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,  # <---\n",
    "    inference_mode=False,\n",
    "    r=4,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules = None if script_args.lora_target_models is None else script_args.lora_target_models.split('|')\n",
    ")\n",
    "\n",
    "model = BaichuanForSequenceClassification.from_pretrained(\n",
    "    script_args.model_name, num_labels=1, torch_dtype=torch.bfloat16,trust_remote_code=True, \n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('baichuan-inc/baichuan-7B', trust_remote_code=True)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['logits', 'past_key_values'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent1 = \"问：我要如何学习新单词并运用它们？\\n\\n答：你会通过阅读和听力学到新的单词，同时也会通过在对话中练习使用这些单词来提高。\"\n",
    "sent2 = \"问：我要如何学习新单词并运用它们？\\n\\n答：你的意思是像“喂，我不知道这些人在说什么，太尴尬了！”或者“这个东西飞在空中，但下方是软的，有什么词能形容它呢？”这样吗？\"\n",
    "output = model(**tokenizer([sent1,sent2],return_tensors='pt',padding=True))\n",
    "output.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0217],\n",
       "         [ 1.3516]], dtype=torch.bfloat16, grad_fn=<ToCopyBackward0>),\n",
       " tensor([[0.4941],\n",
       "         [0.7930]], dtype=torch.bfloat16, grad_fn=<SigmoidBackward0>))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['logits'], torch.sigmoid(output['logits']) # 经过sigmoid之后得到的就是reward分值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.494140625, 0.79296875]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sigmoid(output.logits).view(-1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline('sentiment-analysis',model='nlptown/bert-base-multilingual-uncased-sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_kwargs = {\n",
    "    \"return_all_scores\": True,\n",
    "    \"function_to_apply\": \"none\",\n",
    "    \"batch_size\": 16,\n",
    "    \"truncation\": True,\n",
    "}\n",
    "texts = ['asdfsd','hey whats up','nice try']\n",
    "output = pipe(texts, **sent_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(output[0][0]['score'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge SFT LoRA weights into the Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from peft import PeftModel,get_peft_model\n",
    "import torch\n",
    "# device = torch.device('cuda:0')\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('baichuan-inc/baichuan-7B', trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    'baichuan-inc/baichuan-7B', num_labels=1, \n",
    "    torch_dtype=torch.bfloat16, trust_remote_code=True, \n",
    "    device_map=\"auto\",\n",
    "    # device_map='cuda:0'\n",
    ")\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PeftModel.from_pretrained(model, '../weights/hc3_chatgpt_zh_specific_qa_baichuan-7B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我是一个计算机程序，由人工智能系统训练\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextStreamer\n",
    "from trl import AutoModelForCausalLMWithValueHead\n",
    "\n",
    "ppo_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "    model\n",
    ")\n",
    "\n",
    "def chat(text,generation_kwargs={}):\n",
    "    streamer = TextStreamer(tokenizer,skip_prompt=True,skip_special_tokens=True)\n",
    "\n",
    "    inputs = tokenizer(\"问：\"+text+\"答：\", return_tensors='pt')\n",
    "    inputs = inputs.to('cuda:2')\n",
    "    output = ppo_model.generate(**inputs, repetition_penalty=1.1, streamer=streamer,**generation_kwargs)\n",
    "\n",
    "chat(\"哈喽，你是哪个？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "# 探究一下 generate 参数\n",
    "import numpy as np\n",
    "\n",
    "output_min_length = 8\n",
    "output_max_length = 10\n",
    "max_new_tokens = np.random.choice(list(range(output_min_length, output_max_length)))\n",
    "print(max_new_tokens)\n",
    "generation_kwargs = {\n",
    "    # \"min_length\": -1,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    # \"pad_token_id\": tokenizer.pad_token_id,\n",
    "    # \"eos_token_id\": tokenizer.eos_token_id,\n",
    "    \"max_new_tokens\":512\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id, tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_tensors = [\n",
    "    [31106, 31394,    77, 13502,  4383,  7482, 31218, 31308, 31190,  1197,\n",
    "        14909,  5235,  1681, 31263,  5235, 31865,    75,     5,     5, 31902,\n",
    "           77],\n",
    "    [31106, 31394,    77,  8929, 31166, 32117, 14432, 31347,  7579,  3476,\n",
    "        13940, 26796,  3442,    75,     5,     5, 31902,    77],\n",
    "    [31106, 31394,    77,  8929, 31166,  3476,\n",
    "        3442,    75,     5,     5, 31902,    77]\n",
    "]\n",
    "question_tensors = [torch.tensor(tensor) for tensor in question_tensors]\n",
    "batch = question_tensors\n",
    "batch_mask = [torch.ones_like(element) for element in batch]\n",
    "inputs = {\"input_ids\": batch, \"attention_mask\": batch_mask}\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "padded_inputs = tokenizer.pad(\n",
    "    inputs,\n",
    "    padding=True,\n",
    "    max_length=None,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "# t = tokenizer.decode(question_tensors)\n",
    "# print(t)\n",
    "ppo_model.generate(**padded_inputs,**generation_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaiChuanForCausalLM(\n",
       "  (model): Model(\n",
       "    (embed_tokens): Embedding(64000, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x DecoderLayer(\n",
       "        (self_attn): Attention(\n",
       "          (W_pack): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=64000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "model = model.merge_and_unload()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ckps/baichaun-sft-hc3-merged/tokenizer_config.json',\n",
       " 'ckps/baichaun-sft-hc3-merged/special_tokens_map.json',\n",
       " 'ckps/baichaun-sft-hc3-merged/tokenizer.model',\n",
       " 'ckps/baichaun-sft-hc3-merged/added_tokens.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckp_path = 'ckps/baichaun-sft-hc3-merged'\n",
    "model.save_pretrained(ckp_path)\n",
    "tokenizer.save_pretrained(ckp_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model with Value Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import AutoModelForCausalLMWithValueHead\n",
    "from transformers import AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    'baichuan-inc/baichuan-7B',\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16, device_map='auto'\n",
    ")\n",
    "# base_model_for_PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载 SFT LoRA weights\n",
    "base_model_with_lora = PeftModel.from_pretrained(\n",
    "    base_model, '../weights/hc3_chatgpt_zh_specific_qa_baichuan-7B'\n",
    ")\n",
    "# ppo_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AutoModelForCausalLMWithValueHead(\n",
       "  (pretrained_model): PeftModelForCausalLM(\n",
       "    (base_model): LoraModel(\n",
       "      (model): BaiChuanForCausalLM(\n",
       "        (model): Model(\n",
       "          (embed_tokens): Embedding(64000, 4096, padding_idx=0)\n",
       "          (layers): ModuleList(\n",
       "            (0-31): 32 x DecoderLayer(\n",
       "              (self_attn): Attention(\n",
       "                (W_pack): Linear(\n",
       "                  in_features=4096, out_features=12288, bias=False\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.1, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=4096, out_features=4, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=4, out_features=12288, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                )\n",
       "                (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                (rotary_emb): RotaryEmbedding()\n",
       "              )\n",
       "              (mlp): MLP(\n",
       "                (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "                (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "                (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "                (act_fn): SiLUActivation()\n",
       "              )\n",
       "              (input_layernorm): RMSNorm()\n",
       "              (post_attention_layernorm): RMSNorm()\n",
       "            )\n",
       "          )\n",
       "          (norm): RMSNorm()\n",
       "        )\n",
       "        (lm_head): Linear(in_features=4096, out_features=64000, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (v_head): ValueHead(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (summary): Linear(in_features=4096, out_features=1, bias=True)\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "    base_model_with_lora\n",
    ")\n",
    "ppo_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrained_model.base_model.model.model.layers.0.self_attn.W_pack.lora_A.default.weight False\n",
      "pretrained_model.base_model.model.model.layers.0.self_attn.W_pack.lora_B.default.weight False\n",
      "pretrained_model.base_model.model.model.layers.1.self_attn.W_pack.lora_A.default.weight False\n",
      "pretrained_model.base_model.model.model.layers.1.self_attn.W_pack.lora_B.default.weight False\n",
      "pretrained_model.base_model.model.model.layers.2.self_attn.W_pack.lora_A.default.weight False\n",
      "pretrained_model.base_model.model.model.layers.2.self_attn.W_pack.lora_B.default.weight False\n",
      "pretrained_model.base_model.model.model.layers.3.self_attn.W_pack.lora_A.default.weight False\n",
      "pretrained_model.base_model.model.model.layers.3.self_attn.W_pack.lora_B.default.weight False\n",
      "pretrained_model.base_model.model.model.layers.4.self_attn.W_pack.lora_A.default.weight False\n",
      "pretrained_model.base_model.model.model.layers.4.self_attn.W_pack.lora_B.default.weight False\n",
      "pretrained_model.base_model.model.model.layers.5.self_attn.W_pack.lora_A.default.weight False\n",
      "pretrained_model.base_model.model.model.layers.5.self_attn.W_pack.lora_B.default.weight False\n",
      "pretrained_model.base_model.model.model.layers.6.self_attn.W_pack.lora_A.default.weight False\n",
      "pretrained_model.base_model.model.model.layers.6.self_attn.W_pack.lora_B.default.weight False\n",
      "pretrained_model.base_model.model.model.layers.7.self_attn.W_pack.lora_A.default.weight False\n",
      "pretrained_model.base_model.model.model.layers.7.self_attn.W_pack.lora_B.default.weight False\n",
      "pretrained_model.base_model.model.model.layers.8.self_attn.W_pack.lora_A.default.weight False\n",
      "pretrained_model.base_model.model.model.layers.8.self_attn.W_pack.lora_B.default.weight False\n",
      "pretrained_model.base_model.model.model.layers.9.self_attn.W_pack.lora_A.default.weight False\n",
      "pretrained_model.base_model.model.model.layers.9.self_attn.W_pack.lora_B.default.weight False\n",
      "pretrained_model.base_model.model.model.layers.10.self_attn.W_pack.lora_A.default.weight False\n",
      "pretrained_model.base_model.model.model.layers.10.self_attn.W_pack.lora_B.default.weight False\n",
      "pretrained_model.base_model.model.model.layers.11.self_attn.W_pack.lora_A.default.weight False\n",
      "pretrained_model.base_model.model.model.layers.11.self_attn.W_pack.lora_B.default.weight False\n",
      "pretrained_model.base_model.model.model.layers.12.self_attn.W_pack.lora_A.default.weight False\n",
      "pretrained_model.base_model.model.model.layers.12.self_attn.W_pack.lora_B.default.weight False\n",
      "pretrained_model.base_model.model.model.layers.13.self_attn.W_pack.lora_A.default.weight False\n",
      "pretrained_model.base_model.model.model.layers.13.self_attn.W_pack.lora_B.default.weight False\n",
      "pretrained_model.base_model.model.model.layers.14.self_attn.W_pack.lora_A.default.weight False\n",
      "pretrained_model.base_model.model.model.layers.14.self_attn.W_pack.lora_B.default.weight False\n",
      "pretrained_model.base_model.model.model.layers.15.self_attn.W_pack.lora_A.default.weight False\n",
      "pretrained_model.base_model.model.model.layers.15.self_attn.W_pack.lora_B.default.weight False\n",
      "pretrained_model.base_model.model.model.layers.16.self_attn.W_pack.lora_A.default.weight False\n",
      "pretrained_model.base_model.model.model.layers.16.self_attn.W_pack.lora_B.default.weight False\n",
      "pretrained_model.base_model.model.model.layers.17.self_attn.W_pack.lora_A.default.weight False\n",
      "pretrained_model.base_model.model.model.layers.17.self_attn.W_pack.lora_B.default.weight False\n",
      "pretrained_model.base_model.model.model.layers.18.self_attn.W_pack.lora_A.default.weight False\n",
      "pretrained_model.base_model.model.model.layers.18.self_attn.W_pack.lora_B.default.weight False\n",
      "pretrained_model.base_model.model.model.layers.19.self_attn.W_pack.lora_A.default.weight False\n",
      "pretrained_model.base_model.model.model.layers.19.self_attn.W_pack.lora_B.default.weight False\n",
      "pretrained_model.base_model.model.model.layers.20.self_attn.W_pack.lora_A.default.weight False\n",
      "pretrained_model.base_model.model.model.layers.20.self_attn.W_pack.lora_B.default.weight False\n",
      "pretrained_model.base_model.model.model.layers.21.self_attn.W_pack.lora_A.default.weight False\n",
      "pretrained_model.base_model.model.model.layers.21.self_attn.W_pack.lora_B.default.weight False\n",
      "pretrained_model.base_model.model.model.layers.22.self_attn.W_pack.lora_A.default.weight False\n",
      "pretrained_model.base_model.model.model.layers.22.self_attn.W_pack.lora_B.default.weight False\n",
      "pretrained_model.base_model.model.model.layers.23.self_attn.W_pack.lora_A.default.weight False\n",
      "pretrained_model.base_model.model.model.layers.23.self_attn.W_pack.lora_B.default.weight False\n",
      "pretrained_model.base_model.model.model.layers.24.self_attn.W_pack.lora_A.default.weight False\n",
      "pretrained_model.base_model.model.model.layers.24.self_attn.W_pack.lora_B.default.weight False\n",
      "pretrained_model.base_model.model.model.layers.25.self_attn.W_pack.lora_A.default.weight False\n",
      "pretrained_model.base_model.model.model.layers.25.self_attn.W_pack.lora_B.default.weight False\n",
      "pretrained_model.base_model.model.model.layers.26.self_attn.W_pack.lora_A.default.weight False\n",
      "pretrained_model.base_model.model.model.layers.26.self_attn.W_pack.lora_B.default.weight False\n",
      "pretrained_model.base_model.model.model.layers.27.self_attn.W_pack.lora_A.default.weight False\n",
      "pretrained_model.base_model.model.model.layers.27.self_attn.W_pack.lora_B.default.weight False\n",
      "pretrained_model.base_model.model.model.layers.28.self_attn.W_pack.lora_A.default.weight False\n",
      "pretrained_model.base_model.model.model.layers.28.self_attn.W_pack.lora_B.default.weight False\n",
      "pretrained_model.base_model.model.model.layers.29.self_attn.W_pack.lora_A.default.weight False\n",
      "pretrained_model.base_model.model.model.layers.29.self_attn.W_pack.lora_B.default.weight False\n",
      "pretrained_model.base_model.model.model.layers.30.self_attn.W_pack.lora_A.default.weight False\n",
      "pretrained_model.base_model.model.model.layers.30.self_attn.W_pack.lora_B.default.weight False\n",
      "pretrained_model.base_model.model.model.layers.31.self_attn.W_pack.lora_A.default.weight False\n",
      "pretrained_model.base_model.model.model.layers.31.self_attn.W_pack.lora_B.default.weight False\n"
     ]
    }
   ],
   "source": [
    "for name, param in ppo_model.named_parameters():\n",
    "    # 这里打印出来可以看到所有的带 lora 字眼的参数都在 W_pack 下，因为在前面训练的时候就是这么设置的\n",
    "    # 然后打印出每个参数的 param.requires_grad，可以看到全部都是 False，因为PeftModel加载之后，默认都是 False，参数都是冻结的\n",
    "    # 因此，想让之前的 LoRA 继续训练，就是把 LoRA 相关参数给解冻即可\n",
    "    if 'lora' in name:\n",
    "        print(name, param.requires_grad)\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "from transformers import TextStreamer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ckps/baichaun-sft-hc3-merged\", trust_remote_code=True)\n",
    "import torch\n",
    "device = torch.device('cuda:2')\n",
    "model = AutoModelForCausalLM.from_pretrained(\"ckps/baichaun-sft-hc3-merged\", trust_remote_code=True).to(device)\n",
    "\n",
    "def chat(text):\n",
    "    streamer = TextStreamer(tokenizer,skip_prompt=True,skip_special_tokens=True)\n",
    "\n",
    "    inputs = tokenizer(\"问：\"+text+\"答：\", return_tensors='pt')\n",
    "    inputs = inputs.to('cuda:2')\n",
    "    output = model.generate(**inputs, max_new_tokens=50,repetition_penalty=1.1, streamer=streamer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "model = PeftModel.from_pretrained(model, 'weights/baichaun_rlhf_beyond_chinese_teststep_200')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我是一个计算机程序，由人工智能系统控制。我的目的是帮助人们解决问题和学习新知识。\n"
     ]
    }
   ],
   "source": [
    "chat('嗨，你是谁啊')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gby",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
